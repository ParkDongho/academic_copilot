# 1. Introduction to Neural Networks

클라우드의 대규모 데이터 세트와 이를 구동하는 수많은 컴퓨터 간의 시너지가 기계 학습에 르네상스를 가능하게 했습니다. 특히, 딥 뉴럴 네트워크(DNN)는 전통적인 접근 방식에 비해 음성 인식에서 단어 오류율을 30% 감소시키는 등 20년 만에 가장 큰 성과를 이끌어냈으며 [ref17], 2011년 이래 이미지 인식 대회에서 오류율을 26%에서 3.5%로 낮췄으며 [ref30]–[ref56], [ref22], 사람 챔피언을 바둑에서 이기게 했습니다 [ref53].

뉴럴 네트워크(NN)는 두뇌와 유사한 기능을 목표로 하며, 가중 입력값의 합에 대한 비선형 함수(예: max(0, value))인 단순한 인공 뉴런에 기반합니다. 이러한 가상 뉴런들은 층(layers)으로 모여져 있어서, 하나의 층의 출력이 다음 순서의 층의 입력이 됩니다. DNN의 "깊이" 부분은 몇 개의 층을 넘어서 더 나아가는 것을 의미하며, 클라우드에 있는 대량의 데이터 세트를 통해 더 큰 층을 추가하여 더욱 정확한 모델을 구축할 수 있게 되었습니다. 이는 더 높은 수준의 패턴이나 개념을 포착할 수 있게 하며, GPU<sup>s</sup>는 이를 개발할 수 있는 충분한 계산 능력을 제공합니다.

NN의 두 가지 단계는 학습(또는 교육)과 추론(또는 예측)이라고 하며, 이는 개발과 생산을 대비시킵니다. 개발자는 층의 수와 NN의 유형을 선택하며, 학습 단계에서 가중치를 결정합니다. 오늘날 거의 모든 학습은 부동 소수점으로 이루어지며, 이는 GPU가 인기를 끈 한 가지 이유입니다. 양자화(quantization)라는 단계는 부동 소수점을 좁은 정수-often just 8 bits-로 변환하며, 이는 대개 추론에 충분합니다. 8비트 정수 곱셈은 IEEE 754의 16비트 부동 소수점 곱셈에 비해 에너지가 6배 적고 면적도 6배 작아질 수 있으며, 정수 덧셈의 경우 에너지는 13배, 면적은 38배의 이점을 가집니다 [ref15].

오늘날 세 가지 종류의 NN이 인기를 끌고 있습니다:

1. Multi-Layer Perceptrons (MLP): 각 새로운 층은 이전 층으로부터의 모든 출력의 가중 합의 비선형 함수 집합(완전 연결)입니다.
2. Convolutional Neural Networks (CNN): 각 층은 이전 층의 출력의 공간적으로 인접한 부분 집합에서 서로 다른 좌표의 가중 합의 비선형 함수 집합으로, 가중치를 재활용할 수 있게 합니다.
3. Recurrent Neural Networks (RNN): 각 후속 층은 출력과 이전 상태의 가중 합의 비선형 함수 모음입니다. 가장 인기 있는 RNN은 Long Short-Term Memory(LSTM)입니다. LSTM의 기술은 무엇을 잊고, 무엇을 다음 층에 상태로 전달할지를 결정하는 데 있습니다. 가중치는 시간 단계에 걸쳐 재활용됩니다.

[Table 1](#table1)은 우리가 벤치마크로 사용하는 세 가지 유형의 NN 각각의 두 가지 사례를 보여주며, 이는 데이터센터에서 95%의 NN 추론 작업 부하를 대표합니다. 주로 TensorFlow [ref1]로 작성되며, 그 길이는 놀랍도록 짧습니다: 단지 100에서 1500줄의 코드입니다. 우리의 벤치마크는 호스트 서버에서 실행되는 C++ 코드가 수천에서 수백만 줄이 될 수 있는 더 큰 응용 프로그램의 작은 조각을 나타냅니다. 이러한 응용 프로그램은 일반적으로 사용자 지향적이어서 엄격한 응답 시간 제한을 두게 됩니다.

각 모델에는 5M에서 100M 사이의 가중치가 필요합니다 ([Table 1](#table1)의 9번째 열), 이는 접근하는 데 많은 시간과 에너지가 소요될 수 있습니다. 접근 비용을 완화하기 위해, 같은 가중치가 추론이나 학습 중에 독립적인 예제의 배치(batch)에 걸쳐 재활용되며, 이는 성능을 개선합니다.

이 논문은 Tensor Processing Unit(TPU)을 설명하고 측정하며, 이들의 추론 성능과 전력을 현대의 CPU 및 GPU와 비교합니다. 다음은 하이라이트 미리 보기입니다:

- 추론 앱은 일반적으로 처리량보다 응답 시간을 강조합니다. 이는 주로 사용자에게 향하는 경우가 많기 때문입니다.
- 지연 제한의 결과로, K80 GPU는 Haswell CPU에 비해 추론에 대해 살짝 더 빠르지만, 피크 성능과 메모리 대역폭은 훨씬 더 높습니다.
- 대부분의 설계자들이 CNN을 가속화하고 있지만, 이는 우리 데이터센터 작업 부하의 단지 5%에 불과합니다.
- TPU는 K80 GPU 및 Haswell CPU에 비해 추론에서 약 15X-30X 빠릅니다.
- 6개의 NN 응용 프로그램 중 4개는 메모리에 구속됩니다; 만약 TPU가 K80 GPU와 같은 메모리를 가지도록 수정된다면, GPU 및 CPU보다 약 30X-50X 더 빠를 것입니다.
- 훨씬 더 작고 전력 소모가 적은 칩을 가지고 있음에도 불구하고, TPU는 K80 GPU보다 25배 많은 MACs와 3.5배 많은 온칩 메모리를 가지고 있습니다.
- TPU의 에너지 대비 성능은 현대의 CPU 및 GPU에 비해 30X-80X이며, K80 메모리가 탑재된 수정된 TPU는 70X-200X 더 우수할 것입니다.

# 2. TPU Origin, Architecture, Implement Ation, and Software

2006년 초부터, 우리 데이터센터에 GPU, FPGA 또는 맞춤형 ASIC을 배치하는 것을 논의했다. 특수 하드웨어에서 실행할 수 있는 소수의 애플리케이션은 대규모 데이터센터의 잉여 용량을 사용하여 거의 무료로 처리될 수 있으며, 무료를 능가하기는 어렵다는 결론을 내렸다. 이는 2013년에, 사람들이 음성 인식을 통해 하루에 3분간 음성으로 검색하게 될 경우, 데이터센터의 컴퓨테이션 수요가 두 배로 늘어나게 되어 기존의 CPU를 사용하는 것은 매우 비용이 많이 들 것이라는 예측이 나오면서 변화되었다. 따라서, 우리는 신속하게 추론을 위한 맞춤형 ASIC을 생산하기 위한 높은 우선순위의 프로젝트를 시작하게 되었고, 학습을 위해 기성의 GPU를 구매했다. 목표는 GPU에 비해 비용-성능을 10배 향상시키는 것이었다. 이러한 명령 하에, 불과 15개월 만에 TPU가 설계, 검증[참조55], 구축 및 데이터센터에 배포되었다. (이 논문에서는 TPU에 대한 양과 세부사항이 제한되므로, 더 자세한 내용은 [참조46]–​[참조57], 및 [참조60]을 보라.)

TPU는 기존 서버에 GPU처럼 플러그인 할 수 있도록 PCIe I/O 버스에 공동 프로세서로 설계되었으며, CPU와 밀접하게 통합되지 않았다. 이는 배포 지연 가능성을 줄이기 위해서였다. 또한 하드웨어 설계와 디버깅을 단순화하기 위해, 호스트 서버가 TPU가 실행할 명령을 전송하며, TPU가 명령을 직접 가져오지는 않는다. 그래서 TPU는 GPU보다 FPU(부동소수점 유닛) 공동 프로세서에 더 가깝다.

목표는 2015년 및 그 이후의 신경망 필요에 부합할 수 있도록, 2013년 신경망에 필요한 것에 국한되지 않고 TPU에서 전체 추론 모델을 실행하여 호스트 CPU와의 상호 작용을 줄이는 것이었다. [Figure 1](#fig1)은 TPU의 블록 다이어그램을 보여준다.

TPU 명령은 호스트로부터 PCIe Gen3×16 버스를 통해 명령 버퍼에 전송된다. 내부 블록은 일반적으로 256바이트 너비의 경로로 연결된다. 오른쪽 상단부터 시작하여, Matrix Multiply Unit은 TPU의 핵심이다. 이 장치는 256×256 MAC를 포함하고 있으며, 부호 있는 또는 부호 없는 정수에 대해 8비트의 곱셈 및 덧셈을 수행할 수 있다. 16비트의 산출물은 행렬 유닛 아래의 4 MiB의 32비트 누산기에 저장된다. 4 MiB는 각각 256개의 요소로 구성된 4096개의 32비트 누산기를 포함하고 있다. 행렬 유닛은 클럭 주기당 하나의 256-요소 부분 합을 생성한다. 우리는 최대 성능에 도달하기 위해 바이트당 필요한 연산(섹션 4의 루프라인 무릎) ~1350을 산출한 후 이를 2048로 올림하고, 그런 다음 컴파일러가 최대 성능에서 실행되는 동안 이중 버퍼링을 사용할 수 있도록 복제하여 4096을 선택했다.

![Table 1](../../assets/img/2017-06-24-ieee-8192463/8192463-table-1-source-large.gif)

Table 1.- TPU 작업의 95%를 대표하는 여섯 개의 NN 애플리케이션(각 NN 유형당 2개). 열은 NN 이름; 코드 줄 수; NN의 레이어 유형 및 수 (fc는 연결 완비, conv는 컨볼루션, 벡터는 자명, pool은 tpu에서 비선형 크기 조정을 수행); 비선형 함수; 가중치 수; TPU의 바이트당 연산 수(그림 5의 연산 강도); 배치 크기; 2016년 7월의 TPU 애플리케이션 인기이다. 한 DNN은 rankbrain [14]; 한 LSTM은 GNM 번역 [59]의 하위 세트; 한 CNN은 inception; 다른 CNN은 deepmind alphago [53], [27].

8비트 가중치와 16비트 활성화를 혼합하여 사용할 때 (또는 그 반대), 행렬 유닛은 반속도로 연산하며, 둘 다 16비트일 경우 4분의 1 속도로 연산한다. 이 유닛은 클럭 주기당 256개의 값을 읽고 쓸 수 있으며, 행렬 곱셈이나 컨볼루션을 수행할 수 있다. 행렬 유닛은 한 개의 64 KiB 크기의 타일을 위한 가중치를 저장하며, 이중 버퍼링을 위한 타일을 하나 더 저장하여 타일을 이동하는 데 걸리는 256클럭 사이클을 숨긴다. 이 유닛은 밀도가 높은 행렬을 위해 설계되었다. 출시 시간이 부족하여 희소 아키텍처 지원은 제외되었다. 행렬 유닛을 위한 가중치는 칩 내의 Weight FIFO를 통해 단계적으로 이동하며, 이 FIFO는 8 GiB DRAM인 Weight Memory(추론 시 가중치는 읽기 전용; 8 GiB는 동시에 활성화된 여러 모델을 지원)를 읽는다. Weight FIFO는 타일 4개 깊이로 되어 있다. 중간 결과는 24 MiB 칩 내 Unified Buffer에 저장되어, 이는 행렬 유닛의 입력으로 사용할 수 있다. 프로그래머블 DMA 컨트롤러는 CPU 호스트 메모리와 Unified Buffer 사이의 데이터를 전송한다.

[Figure 2](#fig2)는 TPU 다이의 플로어 플랜을 보여준다. 24 MiB Unified Buffer는 다이의 거의 3분의 1이며, Matrix Multiply Unit은 4분의 1이므로 데이터 경로는 다이의 거의 3분의 2를 차지한다. 24 MiB 크기는 부분적으로 다이의 행렬 유닛 피치에 맞추기 위해 선택되었으며, 짧은 개발 일정으로 인해 컴파일러를 단순화하기 위해 선택되었다(섹션 7 참조). 제어는 단 2%이다. [Figure 3](#fig3)는 기존 서버에 SATA 디스크처럼 삽입되는 TPU의 인쇄 회로 기판을 보여준다.

명령은 상대적으로 느린 PCIe 버스를 통해 전송되므로, TPU 명령어는 반복 필드를 포함하여 CISC 전통을 따른다. 이러한 CISC 명령의 명령어당 평균 클럭 사이클(CPI)은 일반적으로 10에서 20 사이이다. 전체적으로 약 12개의 명령이 있지만, 다음 다섯 가지가 주요 명령이다:

### 1. Read_Host_Memory 

`Read_Host_Memory`는 CPU 호스트 메모리에서 데이터를 가져와 Unified Buffer (UB)에 저장합니다.

### 2. Read_Weights 

`Read_Weights`는 Weight Memory에서 가중치를 읽어오고 Matrix Unit의 입력으로 Weight FIFO에 전달합니다.

### 3. MatrixMultiply/Convolve 

`MatrixMultiply/Convolve` 명령은 Unified Buffer에서 Accumulator로의 행렬 곱셈 또는 컨볼루션을 수행하도록 Matrix Unit에 지시합니다. 행렬 연산은 B\*256 크기의 입력에 대해 256×256 크기의 상수 가중치를 곱하여 B*256 출력 결과를 생성합니다. 이 연산은 B 파이프라인 사이클을 거쳐 완료됩니다.

### 4. Activate 

`Activate`는 인공 뉴런의 비선형 함수를 수행하며, ReLU, Sigmoid 등의 옵션을 제공합니다. 이 과정의 입력은 Accumulator이며, 출력은 Unified Buffer입니다. 또한, 전용 하드웨어를 사용하여 컨볼루션에 필요한 pooling 연산도 수행할 수 있습니다.

### 5. Write_Host_Memory 

`Write_Host_Memory`는 Unified Buffer의 데이터를 CPU 호스트 메모리에 쓰는 작업을 수행합니다.

나머지 명령들은 대체 호스트 메모리 읽기/쓰기, 설정 구성, 두 버전의 동기화, 호스트 인터럽트, 디버그 태그, nop, and halt 등이 포함됩니다. CISC `MatrixMultiply` 명령은 12바이트의 크기를 가지며, 이 중 3바이트는 Unified Buffer 주소; 2바이트는 Accumulator 주소; 4바이트는 길이 (컨볼루션의 두 차원에 대한 정보); 나머지는 opcode와 flags 입니다.

---

![Figure 1](../../assets/img/2017-06-24-ieee-8192463/8192463-fig-1-source-large.gif)

**Figure 1. - TPU block diagram.**  
주요 연산은 노란색으로 표현된 행렬 곱셈 유닛에서 이뤄집니다. 입력은 파란색으로 표시된 weight FIFO와 Unified Buffer이고 출력은 파란색으로 표현되는 accumulators입니다. 노란색 활성화 유닛은 Accumulator에 대해 비선형 함수를 수행하며, 결과값은 Unified Buffer로 전달됩니다.

---

![Figure 2](../../assets/img/2017-06-24-ieee-8192463/8192463-fig-2-source-large.gif)

**Figure 2. - Floorplan of TPU die.**  
셰이딩은 그림 1을 따릅니다. 밝은 파란색으로 표현된 데이터 경로는 67%, 중간 초록색으로 표현된 입출력은 10%, 어두운 빨간색으로 표현된 제어는 다이의 2%를 차지합니다. 제어부는 CPU나 GPU에서는 더 크고 설계가 훨씬 복잡합니다.

---

![Figure 3](../../assets/img/2017-06-24-ieee-8192463/8192463-fig-3-source-large.gif)

**Figure 3. - TPU printed circuit board.**  
서버 내 SATA 디스크 슬롯에 삽입할 수 있습니다.

---

TPU 마이크로아키텍처의 철학은 행렬 유닛을 최대한 바쁘게 유지하는 것입니다. 이 CISC 명령을 위해 4단계 파이프라인을 사용하여 각 명령은 별도의 단계에서 실행됩니다. 다른 명령의 실행을 MatrixMultiply 명령과 중첩시켜 실행을 숨기는 계획이었습니다. 이를 위해 Read_Weights 명령은 decoupled-access/execute 철학을 따릅니다. (*ref54*) 이 원칙에 따라 그것은 주소를 보낸 후 Weight Memory에서 가중치를 가져오기 전에 완료할 수 있습니다. 만약 입력 활성화나 가중치 데이터가 준비되지 않으면 행렬 유닛은 멈추게 됩니다.

전통적인 RISC 파이프라인(각 단계당 하나씩)과 달리, CISC 명령은 수천의 클럭 사이클 동안 스테이션을 차지할 수 있기 때문에 명확한 파이프라인 중첩 다이어그램은 없습니다. 한 네트워크 층의 활성화가 완료되어야 다음 층의 행렬 곱셈이 시작될 수 있는 상황이 발생합니다. 이는 Unified Buffer에서 안전하게 읽기 전에 명시적 동기화를 기다리는 RAW 파이프라인 스톨을 보여 줍니다.

큰 SRAM을 읽는 것은 산술 연산보다 훨씬 더 많은 전력을 필요로 하기 때문에, 행렬 유닛은 Unified Buffer의 읽기와 쓰기를 줄여 에너지를 절약하는 systolic 실행을 사용합니다 (*ref31*–​*ref44*, *ref40*). 이 방식은 배열 내 셀에 주기적으로 도달하는 다른 방향의 데이터에 의존하여, 이러한 데이터가 결합됩니다. [Figure 4](#fig4)는 데이터가 왼쪽에서 들어오고 가중치는 위쪽에서 불러오는 과정을 보여줍니다. 주어진 256 엘리먼트의 multiply-accumulate 연산은 대각선 파형으로 행렬을 통과합니다. 가중치는 사전에 준비되어 있으며, 새로운 블록의 첫 번째 데이터와 함께 진행하는 파형과 함께 효과적으로 작동합니다. 제어와 데이터는 파이프라인화되어 256개의 입력이 동시에 읽히고 즉시 256개의 Accumulator의 한 위치를 업데이트하는 착각을 줍니다. 정확성 프레임워크에서 소프트웨어는 행렬 유닛의 systolic 특성을 인식하지 못하지만, 성능을 위해서는 유닛의 레이턴시에 대해 걱정합니다.

TPU 소프트웨어 스택은 CPU 및 GPU용으로 개발된 것들과 호환되어야 하며, 어플리케이션을 TPU로 빠르게 포팅할 수 있어야 합니다. TPU에서 실행되는 어플리케이션의 일부는 일반적으로 TensorFlow로 작성되며 GPU나 TPU에서 실행할 수 있는 API로 컴파일됩니다 (*ref33*). GPU와 마찬가지로 TPU 스택은 사용자 공간 드라이버와 커널 드라이버로 나뉩니다. 커널 드라이버는 경량이며 메모리 관리와 인터럽트만 처리하며, 장기적 안정성을 위해 설계되었습니다.

사용자 공간 드라이버는 자주 변경됩니다. TPU 실행을 설정하고 제어하며, 데이터를 TPU 순서로 재포맷하고 API 호출을 TPU 명령으로 번역해 애플리케이션 바이너리로 변경합니다. 사용자 공간 드라이버는 모델을 처음 평가할 때 컴파일하고, 프로그램 이미지를 캐시하며 가중치 이미지를 TPU의 가중치 메모리에 기록합니다. 두 번째 및 이후 평가에서는 최대 속도로 실행됩니다. TPU는 대부분의 모델을 입력에서 출력까지 완전하게 실행하며, TPU 연산 시간과 I/O 시간의 비율을 극대화합니다. 계산은 종종 한 층씩 이루어지며, 겹쳐진 실행을 통해 행렬 곱셈 유닛이 대부분의 비핵심 경로 오퍼레이션을 숨길 수 있도록 합니다.

# 3. CPU, GPU, and TPU Platforms

[Table 1](#table1)의 여섯 가지 프로덕션 애플리케이션이 이 논문의 워크로드입니다. 위에서 언급했듯이, 이 여섯 가지 애플리케이션은 데이터센터에서 TPU 사용의 95%를 대표합니다. 아이러니하게도, AlexNet이나 VGG와 같은 소형 DNN을 프로덕션 머신에서 배포하고 측정하는 것은 어렵습니다. 그러나, 우리의 CNN 중 하나는 널리 사용되는 Inception V2에서 파생되었습니다.

벤치마크 플랫폼은 TPU가 배포된 2015년에 사용 가능했던 서버급 컴퓨터들입니다. 이 제한으로 인해 이들은 TPU와 같이 내부 SRAM과 외부 DRAM 메모리의 SECDED 보호를 최소한으로 포함해야 했고, 이는 Nvidia Maxwell GPU와 같은 몇 가지 선택사항을 제외했습니다. 우리 회사가 구매하고 배포하기 위해서는 또한 벤치마크를 단지 승리하기 위해 조립된 어색한 아티팩트가 아닌, 합리적으로 구성된 머신이어야 했습니다.

[Table 2](#table2)는 우리의 선택을 나열하고 있습니다. 전통적인 CPU 서버는 Intel의 18코어, 듀얼 소켓 Haswell 프로세서로 대표됩니다. 이 플랫폼은 또한 GPU나 TPU의 호스트 서버입니다. Haswell은 Intel 22nm 공정에서 제작되었습니다. CPU와 GPU 둘 다 매우 큰 다이를 가지고 있으며, 크기는 약 600 mm<sup>2</sup>입니다.

<a name="fig4"></a>
![Systolic data flow of the matrix multiply unit.](../../assets/img/2017-06-24-ieee-8192463/8192463-fig-4-source-large.gif)

Figure 4. - 행렬 곱셈 유닛의 시스템적 데이터 흐름.

<a name="table2"></a>
![Benchmarked servers use Haswell CPUs, K80 GPUs, and TPUs.](../../assets/img/2017-06-24-ieee-8192463/8192463-table-2-source-large.gif)

Table 2.- 벤치마크된 서버는 Haswell CPU, K80 GPU, 및 TPU를 사용합니다. Haswell은 18코어를 가지고 있으며, K80은 13개의 SMX 프로세서를 가지고 있습니다. Figure 10은 측정된 전력을 보여줍니다. 저전력 TPU는 고전력 GPU보다 더 나은 랙 수준의 밀도를 제공합니다. TPU 당 8 gib DRAM은 가중치 메모리입니다. GPU 부스트 모드는 사용되지 않습니다 (Section 8 참고). SECDED와 부스트 모드 부재로 인해 K80 대역폭은 광고된 240에서 160 gb/s로 감소합니다. 부스트 모드와 단일 다이 대 듀얼 다이 성능은 광고된 K80 피크 tops/s를 8.7에서 2.8로 감소시킵니다. (*TPU 다이는 Haswell 다이 크기의 절반 이하입니다.)

2.3 GHz CPU 클럭 속도는 NN 애플리케이션에서 우리 데이터센터에 거의 발생하지 않기 때문에 터보 모드를 포함하지 않습니다. Haswell은 프로그램이 AVX 명령을 사용하는지 여부에 따라 다른 클럭 속도를 가지고 있으며, 우리의 NN 애플리케이션은 종종 이를 사용합니다. 터보 모드의 높은 클럭 속도(AVX를 피하는 프로그램의 경우)는 모든 코어를 사용하지 않을 때 발생합니다. 따라서, 우리의 애플리케이션이 일반적으로 모든 코어를 사용하며, 유휴 코어를 채우기 위해 다른 데이터 센터 작업을 실행할 수 있으므로, 터보 모드는 우리의 데이터센터에서 드뭅니다.

GPU 가속기는 Nvidia K80입니다. 각 K80 카드에는 두 개의 다이가 있으며 내부 메모리와 DRAM에 SECDED를 제공합니다. Nvidia는 “K80 Accelerator가 소수의 더 강력한 서버로 애플리케이션 성능을 제공하여 데이터 센터 비용을 극적으로 낮춘다”고 설명합니다 [ref38]. 2015년에는 NN 연구자들이 K80을 자주 사용했으며, 최근인 2016년 9월에도 새로운 클라우드 기반 GPU 제공을 위해 선택되었습니다 [ref7]. 이 서버에는 최대 8개의 K80 다이를 4장의 카드에 설치할 수 있으며, 이것이 우리가 벤치마크를 수행한 구성입니다.

서버 당 다이 수가 2에서 8까지 다양하기 때문에, 우리는 보통 다이 당 정규화된 결과를 보여줍니다 ([Figures 5](#fig5)–​[8](#fig8), [Figures 10](#fig10) [11](#fig11), 그리고 [Tables 3](#table3), [5](#fig5)와 [7](#fig7)), 그러나 때때로 전체 시스템을 보여주기도 합니다 ([Figure 9](#fig9)). 이 구별이 명확하길 바랍니다.

# 4. Performance: Rooflines, Response-Time, and Throughput

세 프로세서에서 여섯 개의 앱 성능을 설명하기 위해, 우리는 고성능 컴퓨팅(HPC) [ref58]에서 Roofline Performance 모델을 적용했습니다. 이 간단한 시각적 모델은 완벽하지는 않지만 성능 병목 현상의 원인을 이해하는 데 도움을 줍니다. 이 모델의 전제는 애플리케이션이 온칩 캐시에 들어가지 않으므로 계산 제한 또는 메모리 대역폭 제한이라는 것입니다. HPC의 경우, Y축은 초당 부동 소수점 연산 성능을 나타내며, 따라서 최대 계산 속도가 Roofline의 "평평한" 부분을 형성합니다. X축은 DRAM 바이트당 부동 소수점 연산 수로 측정되는 연산 강도(operational intensity)입니다. 메모리 대역폭은 초당 바이트로, (FLOPS/sec)/(FLOPS/Byte) = Bytes/sec 관계식에 의해 Roofline의 "기울어진" 부분이 됩니다. 충분한 연산 강도가 없으면 프로그램은 메모리 대역폭에 의해 제한되며 Roofline의 기울어진 부분 아래에 위치하게 됩니다.

애플리케이션의 실제 초당 연산 수와 바로 위에 있는 Ceiling 사이의 차이는 추가 성능 튜닝의 잠재적 이점을 보여주며, 연산 강도를 그대로 두었을 때 얻을 수 있는 이점입니다. 물론, 연산 강도를 증가시키는 최적화(예: 캐시 블로킹)는 더 큰 이점을 가져올 수 있습니다.

TPU에 Roofline 모델을 적용하려면 NN 애플리케이션이 양자화될 때 먼저 부동 소수점 연산을 정수 연산으로 교체합니다. 일반적으로 NN 애플리케이션의 경우 가중치가 온칩 메모리에 맞지 않으므로 두 번째로 연산 강도를 읽은 가중치 바이트당 정수 연산량으로 재정의합니다([Table 1](#table1)의 열 10 참조).

[Figure 5](#fig5)는 로그-로그 스케일에서 단일 TPU 다이의 Roofline 모델을 보여줍니다. TPU는 연산 강도가 메모리 대역폭에 의해 성능이 제한되는 긴 "기울어진" 부분의 Roofline을 가지고 있습니다. 여섯 개 애플리케이션 중 다섯 개는 행복하게 천장을 치고 있습니다: MLP와 LSTM은 메모리 제한을 받고, CNN은 계산 제한을 받습니다. CNN1은 높은 연산 강도를 가지지만 14.1 TOPS로 실행되며, CNNO는 86 TOPS로 실행됩니다.

[Table 3](#table3)은 CNN1의 경우 TPU 작업에 부분적으로 관여하는 성능 카운터 기반으로 상황을 설명합니다. TPU는 CNN1에 대해 행렬 연산을 수행하는 데 전체 사이클의 절반 이하를 소비합니다(열 7, 행 1). 이러한 활성 사이클 각각에서 CNN1의 몇몇 레이어는 피처 깊이가 얕기 때문에 65,536 MAC 중 약 절반만이 유용한 가중치를 보유합니다. 약 35%의 사이클은 메모리에서 행렬 유닛으로 가중치가 로드되는 것을 기다리는 시간으로, 연산 강도가 32에 불과한 네 개의 완전 연결 레이어 동안 발생합니다([Section 8](#sec8)에서의 마지막 오류 참조). 이는 행렬 관련 카운터로 설명되지 않는 약 19%의 사이클을 남깁니다. TPU의 중첩된 실행(overlapped execution)로 인해 정확한 사이클 계정은 없지만, 23%의 사이클이 파이프라인의 RAW 종속성으로 인해 정지하며 1%는 PCIe 버스의 입력 대기로 인한 정지에 사용됨을 확인할 수 있습니다.

[Figure 6](#fig6)과 [7](#fig7)은 단일 Haswell 다이와 단일 K80 다이에 대한 Roofline을 보여줍니다. 여섯 개 NN 애플리케이션은 일반적으로 TPUs의 [Figure 5](#fig5)보다 그들의 Ceiling 아래에 더 멀리에 위치합니다. 이는 응답 시간 때문입니다. 이 NN 애플리케이션 중 많은 부분이 최종 사용자 서비스를 구성합니다. 연구자들은 응답 시간이 약간 증가하면 고객이 서비스를 덜 사용한다고 증명했습니다[ref51]. 따라서 교육은 명확한 응답 시간 제한을 가지지 않을지라도 추론은 그렇습니다. 즉, 추론은 처리량보다 지연 시간을 선호합니다.

예를 들어, MLP0의 99번째 백분위 응답 시간 제한은 7 ms였으며 이는 애플리케이션 개발자가 요구한 바입니다. (초당 추론 수와 7 ms 지연 시간은 서버 호스트 시간과 가속기 시간을 포함합니다.) [Table 4](#table4)는 Haswell과 K80이 MLP0에 대한 응답 시간 제한이 완화될 경우 달성이 가능한 최대 처리량의 각각 42%와 37%로 실행됨을 보여줍니다. 이러한 제한은 TPU에도 영향을 미치지만, 80%에서 이는 MLP0의 최대 처리량에 훨씬 더 가깝게 운영되고 있습니다. CPU와 GPU와 비교할 때, 단일 스레드 TPU는 99번째 백분위 케이스가 아닌 평균 케이스를 개선하기 위해 트랜지스터와 에너지를 소모하는 복잡한 마이크로아키텍처 기능이 없습니다: 캐시, 분기 예측, 비순차 실행, 멀티프로세싱, 추측적 사전 로드, 주소 병합, 멀티스레딩, 컨텍스트 전환 등이 없습니다. 최소주의는 도메인별 프로세서의 장점입니다.

Figure 5. - TPU (die) Roofline
TPU의 Ridge Point는 1350 multiply-accumulate operations per byte of weight memory read에서 오른쪽으로 멀리 떨어져 있습니다. 두 개의 LSTMs와 두 개의 MLPs는 Roofline의 기울어진 부분 아래에 위치하여 TPU에서 메모리 대역폭으로 인해 병목현상이 발생합니다. 두 개의 CNNs는 최고 연산률에 의해 제한을 받습니다.

[Table 3](#table3)은 하드웨어 성능 카운터를 기반으로 한 NN 작업 부하의 TPU 성능 제한 요소를 보여줍니다. 1, 4, 5, 6행은 매트릭스 유닛의 활동 측정을 기반으로 하며, 총 100%를 차지합니다. 2, 3행은 매트릭스 유닛의 64K 가중치 중 유용한 가중치를 보유하는 활동 주기의 비율을 세분화합니다. 우리의 카운터로는 6행에서 매트릭스 유닛이 유휴 상태인 시간을 정확히 설명할 수 없으며, 7, 8행은 RAW 파이프라인 위험과 PCIe 입력 정지를 포함한 두 가지 가능한 이유의 카운터를 보여줍니다. 9행(특정 코드)은 생산 코드의 측정에 기반하고 다른 행은 성능 카운터 측정에 기반하여 완벽하게 일치하지 않을 수 있습니다. 호스트 서버 오버헤드는 여기서 제외되었습니다. CNN1 결과는 본문에서 설명되고 있습니다.

[Table 3](#table3)은 TPU 성능을 보여주지만 호스트 서버의 시간을 고려하지 않으며, 이는 애플리케이션의 호스트 부분을 실행하고 TPU와 통신하는 시간으로 나눌 수 있습니다. [Table 5](#table5)는 두 번째 부분을 나열하지만 첫 번째 부분은 어렵습니다. 큐잉 이론에 따르면 긴 입력 큐는 컴퓨터가 유휴 상태가 되지 않도록 하여 처리량을 증가시키지만 응답 시간을 늘립니다. 따라서 대부분의 애플리케이션은 입력 큐를 비어 있도록 유지합니다. 불행히도, TPU가 애플리케이션의 일부를 수행하기 위해 CPU를 기다리는 경우 또는 입력 큐가 비어 있어 CPU도 유휴인 경우 TPU가 유휴 상태인 시점을 측정할 수 없습니다.

[Table 6](#table6)은 두 가속기와 CPU에 대한 호스트 서버 오버헤드를 포함하여 다이에 대한 상대 추론 성능의 최종 지표를 제공합니다. 마지막에서 두 번째 열은 6개의 NN 애플리케이션에 대한 상대 성능의 기하 평균을 보여주며, 이는 K80 다이가 Haswell 다이보다 1.1배 빠르며, TPU 다이는 14.5배 더 빠르고, 따라서 TPU 다이는 GPU 다이보다 13.2배 더 빠르다는 것을 시사합니다. [Figure 8](#fig8)은 시각적으로 이들의 상대 속도를 보여줍니다.

Figure 6. - Intel Haswell CPU (die) Roofline
Ridge Point가 13 multiply-accumulate operations/byte로, Figure 5보다 왼쪽에 위치하고 있습니다. Lstm0과 mlp1은 Haswell에서 k80보다 빠르지만 다른 DNNs에서는 반대입니다. 응답 시간이 DNN 성능을 제한합니다 (Table 4).

Figure 7. - NVIDIA K80 GPU Die Roofline
높은 메모리 대역폭은 Ridge Point를 9 operations per weight byte로 이동시켜, Figure 6보다 왼쪽에 위치합니다. DNNs는 응답 시간 한도(Table 4)로 인해 Roofline에서 멀리 떨어져 있습니다.

[Table 4](#table4)는 배치 크기가 mlpo에 대해 변할 때 mlp0의 99번째 % 응답 시간과 각 다이에 대한 처리량(ips)을 보여줍니다. 가장 긴 허용 대기 시간은 7ms입니다. GPU와 TPU에서는 호스트 서버의 오버헤드로 인해 mlp0 최대 처리량이 제한됩니다. 큰 배치 크기는 처리량을 증가시키지만, 본문에서 설명하듯이 긴 응답 시간이 한도를 초과하여 CPU와 GPU는 비효율적인 작은 배치 크기(16 vs. 200)를 사용해야 합니다. 이 경우 응답 시간이 무한정인 경우보다 2.3배~2.7배 느리게 실행됩니다. 그러나 더 결정적인 TPU의 경우 99번째 % 응답 시간 제한으로 인한 속도 저하는 단지 1.2배에 불과합니다.

[Table 5](#table5)는 TPU 성능 카운터로부터 TPU 실행 시간의 퍼센트로 표현한 호스트 CPU가 TPU와 상호 작용하는 시간을 보여줍니다. 이 비율은 CPU가 애플리케이션의 일부를 실행하지만 TPU와 상호 작용하지 않는 시간을 포함하지 않고, CPU와 TPU가 pcie 버스를 통해 통신하는 시간입니다. 본문에서 설명하듯이, TPU가 CPU가 유휴 상태인지 또는 애플리케이션을 작업 중인지 측정하기 어려운 것은 사실입니다.

아키텍트들은 실행될 프로그램의 실제 조합을 모르기 때문에 기하 평균을 사용합니다 [ref23]. 그러나 이 연구에서는 조합을 알고 있습니다 ([Table 1](#table1)).  [Table 6](#table6)의 마지막 열에 실제 조합을 사용하여 계산한 가중 평균은 GPU가 1.9배, TPU가 29.2배로 증가시킵니다. 따라서 이제 TPU 다이는 GPU 다이보다 15.3배 빠릅니다.

Figure 8. - Figures 5–7 Combined
로그-로그 그래프로 결합된 그림입니다. 별은 TPU를, 삼각형은 k80을, 원은 haswell을 나타냅니다. 모든 TPU 별은 다른 두 개의 Roofline에서 위에 있습니다.

[Table 6](#table6)은 NN 워크로드에 대한 K80 GPU 다이와 TPU 다이의 CPU에 대한 상대 성능을 보여줍니다. GM과 WM은 기하 평균과 가중 평균( Table 1의 조합 사용)입니다. GPU와 TPU의 상대 성능에는 호스트 서버의 오버헤드가 포함됩니다.

# 5. Cost-Performance, TCO, and Performance/Watt

수천 대의 컴퓨터를 구매할 때는 성능보다 비용 대비 성능이 더 중요할 수 있습니다. 데이터센터에서 가장 중요한 비용 지표는 총 소유 비용(TCO, Total Cost of Ownership)입니다. 수천 개의 칩을 구매할 때 실제 지불 가격은 관련 기업 간의 협상에 따라 달라집니다. 비즈니스상의 이유로 그러한 가격 정보나 추론할 수 있는 데이터를 공개할 수 없습니다. 그러나 전력 소비량은 TCO와 연관되어 있으며, 우리는 서버당 전력 소비(Watts)를 공개할 수 있으므로, 이 논문에서는 성능/TCO의 대용 지표로 성능/와트(Performance/Watt)를 사용합니다. 이 섹션에서는 단일 칩이 아닌 전체 서버를 비교하며, [Table 2](#table2)에는 "Benchmarked Server" 열에 해당 정보가 나열되어 있습니다.

[Figure 9](#fig9)는 Haswell CPU에 비해 K80 GPU와 TPU의 기하 평균 및 가중 평균 성능/와트를 보여줍니다. 성능/와트의 두 가지 다른 계산을 제시합니다. 첫 번째 방법("total")은 GPU와 TPU의 성능/와트를 계산할 때 호스트 CPU 서버에서 소비하는 전력을 포함합니다. 두 번째 방법("incremental")은 GPU와 TPU에서 호스트 CPU 서버 전력을 미리 제외한 것입니다.

총 성능/와트(total-performance/Watt)로는, K80 서버가 Haswell에 비해 1.2배에서 2.1배 더 높은 성능을 보입니다. 호스트 서버 전력을 제외한 증가분 성능/와트(incremental-performance/Watt)로는 K80 서버가 1.7배에서 2.9배 더 높은 성능을 보입니다. TPU 서버는 Haswell보다 17배에서 34배 더 나은 총 성능/와트를 지니며, 이는 TPU 서버가 K80 서버의 성능/와트보다 14배에서 16배 더 우수하다는 것을 의미합니다. 상대적 증가분 성능/와트는, 이는 커스텀 ASIC(주문형 칩)을 개발하는 결정적 이유가 되었으며, TPU에서 41배에서 83배로 GPU보다 25배에서 29배 더 높은 성능/와트를 제공합니다.

![Figure 9](../../assets/img/2017-06-24-ieee-8192463/8192463-fig-9-source-large.gif)

Figure 9. - CPU 서버에 대한 GPU 서버(파란색)와 TPU 서버(빨간색)의 상대적 성능/와트(tdp), 그리고 GPU 서버에 대한 TPU 서버(주황색)의 상대적 값입니다. TPU'는 GDDR5 메모리를 사용하는 개선된 TPU입니다 (자세한 내용은 Section 7를 참조하세요). 초록색 바는 CPU 서버에 대한 비율을, 라벤더색 바는 GPU 서버와의 관계를 나타냅니다. "Total"은 호스트 서버 전력을 포함하고, "Incremental"은 포함하지 않습니다. GM과 WM은 기하 평균과 가중 평균을 의미합니다.

# 6. Energy Proportionality

Thermal Design Power (TDP)는 하드웨어가 최대 전력 상태일 때 충분한 전력 및 냉각을 공급해야 하기 때문에 전력 공급에 드는 비용에 영향을 미칩니다. 그러나 전기 요금은 하루 동안 워크로드가 변동하면서 소비되는 평균 전력을 기준으로 결정됩니다. [ref6]에서는 서버가 10% 미만의 시간 동안 100% 바쁨 상태에 있으며, 에너지 비례성을 주장했습니다: 서버는 수행된 작업의 양에 비례하여 전력을 소비해야 합니다. 이전 섹션에서 소비된 전력의 추정치는 당사 데이터 센터에서 관찰된 TDP의 비율을 기반으로 하고 있습니다.

우리는 서버의 성능과 전력을 CPU, TPU, GPU를 포함하여 제공된 워크로드 활용도가 0%에서 100%까지 변동할 때 측정하고, 워크로드 10% 델타씩 수집했습니다 [ref32]. [Figure 10](#fig10)은 CNN0의 워크로드를 변동시키면서 세 가지 칩의 서버 전력을 서버당 다이 수로 나눈 값을 보여줍니다.

TPU는 다이당 40W로 가장 낮은 전력을 소모하지만, 에너지 비례성이 좋지 않습니다: 10% 부하 시 TPU는 100% 부하에서 사용하는 전력의 88%를 사용합니다. (짧은 설계 일정으로 인해 많은 에너지 절약 기능을 포함하지 못했습니다.) 놀랍지 않게도, Haswell이 그룹 내 에너지 비례성에서 가장 뛰어납니다: 10% 부하에서 100% 부하와 동일한 전력의 56%를 사용합니다. K80은 TPU보다 CPU에 가까워 10% 워크로드에서 전체 부하 전력의 66%를 사용합니다. 계산에 구속받지 않는 LSTM1도 유사한 성능을 보입니다: 10% 부하에서 CPU는 전체 전력의 47%, GPU는 78%, TPU는 94%를 사용합니다.

CNNO를 가속기의 호스트로 만들 경우 서버 전력 사용량에 어떤 변화가 있을까요? GPU와 TPU가 100% 부하에서 작동할 때, CPU 서버는 GPU에 대해 전체 전력의 52%, TPU에 대해 69%를 사용합니다. (CPU는 GPU보다 훨씬 빠르게 실행되기 때문에 TPU에 대해 더 많은 작업을 수행합니다.) 결과적으로, Haswell 서버와 4개의 TPU는 20% 미만의 추가 전력을 사용하지만 Haswell 서버 단독으로 CNNO를 실행할 때보다 80배 빠릅니다 (4 TPUs 대 2 CPUs).

[Figure 10](#fig10): CNN0의 활용도가 0%에서 100%까지 변동할 때 다이당 와트 값. 서버는 2개의 CPU와 8개의 GPU 또는 4개의 TPU를 가지며, 각각 2, 8, 4로 나누어 전력을 정규화합니다.

[Figure 11](#fig11): 메트릭이 0.25x에서 4x까지 확장될 때 가중 평균 TPU 성능: 메모리 대역폭 (memory), 클럭 속도 + 누산기 (clock+), 클럭 속도 (clock), 행렬 유닛 치수 + 누산기2 (matrix+) 및 단독 행렬 유닛 (matrix).

[Table 7](#table7): TPU 하드웨어 성능 카운터와 TPU 성능 모델 간의 클럭 사이클 차이. 평균 델타는 8%입니다.

[Table 8](#table8): NN 애플리케이션당 사용된 24 mib 통합 버퍼의 최대 mib. 새로운 소프트웨어 할당기의 개선 덕분에 현재는 14 mib 통합 버퍼로 충분합니다.

# 7. Evaluation of Alternative TPU Designs

TPU 보조 프로세서는 FPU처럼 상대적으로 평가하기 쉬워 여섯 가지 응용 프로그램에 대한 성능 모델을 생성했습니다. [Table 7](#table7)은 모델 결과와 하드웨어 성능 카운터 간의 차이를 보여주며, 평균적으로 10% 미만입니다. 그런 다음 메모리 대역폭, 클록 속도 및 누산기의 수, 행렬 곱셈 유닛 크기를 다양화하여 성능을 모델링했습니다.

[Figure 11](#fig11)은 0.25배에서 4배 범위로 이러한 매개변수를 확장하면서 TPU 다이의 평균 성능 민감도를 보여줍니다. 가중 평균이 표시되지만 기하 평균도 유사하게 보입니다. 클록 속도만 증가시키는 것(clock in [Figure 11](#fig11))의 영향을 평가하는 것 외에도, 클록 속도를 증가시키고 컴파일러가 더 많은 메모리 참조를 처리할 수 있도록 누산기의 수를 비례적으로 조정한 디자인(clock+)도 표시합니다. 마찬가지로 행렬의 양쪽 차원에서 행렬 곱셈기의 수가 증가하므로 누산기의 수를 제곱하여 증가시키면 행렬 유닛 확장(matrix+)을 나타내고, 행렬 유닛만 증가하는 경우(matrix)를 나타냅니다.

먼저, 메모리 대역폭을 증가시키는 것(memory)이 가장 큰 영향을 미칩니다: 메모리가 4배 증가하면 성능이 평균적으로 3배 향상됩니다. 둘째, 클록 속도는 배치된 누산기의 수와 관계없이 평균적으로 큰 이점을 주지 않습니다. 그 이유는 MLP와 LSTM이 메모리에 의해 제약받고 CNN만이 계산에 의해 제약받기 때문입니다. 모든 여섯 개의 DNN에 대한 가중 평균만 보여주고 있기 때문에 [Figure 11](#fig11)에서 볼 수는 없지만, 클록 속도를 4배 증가시키면 MLP와 LSTM에는 거의 영향을 미치지 않지만 CNN의 성능은 약 2배 향상됩니다. 셋째, [Figure 11](#fig11)에서 행렬 유닛이 256×256에서 512×512로 확장되면 모든 응용 프로그램에서 누산기의 수와 관계없이 평균 성능이 약간 저하됩니다. 이것은 대형 페이지의 내부 조각화와 비슷한 문제로 두 차원에서 발생하기 때문에 더 심각합니다. LSTM1에서 사용되는 600×600 행렬을 고려해 보세요. 256×256 행렬 유닛을 사용하면 600×600을 타일링하기 위해 9단계가 필요하고 총 18 us의 시간이 걸립니다. 더 큰 512×512 유닛은 4단계만 필요하지만 각 단계는 4배 더 긴 시간이 소모되어 총 32 us가 걸립니다. CISC 명령어는 길어 디코드는 비중이 적고 DRAM에서 로드하는 오버헤드를 감추지 못합니다.

[Table 8](#table8)은 24 MiB의 Unified Buffer 활용도를 보여주며, 처음에는 MLP가 최대 2048의 배치 크기로 실행될 수 있도록 크기가 조정되었습니다. 우리는 최근 Unified Buffer의 저장 할당기를 개선하여 여섯 개의 응용 프로그램 중 가장 큰 것에 대해 필요한 메모리를 14 MiB로 줄였습니다. 배포 초반 18개월 동안 새로운 할당기가 개발되는 동안 TPU는 풀 용량을 사용했습니다. 이제 추가 용량이 더 큰 모델 채택을 위한 여유를 제공합니다.

그 후, 우리는 같은 프로세스 기술로 설계될 수 있었던 가상 TPU 다이(TPU’)를 평가하기 위해 성능 모델을 사용했습니다. 더 공격적인 로직 합성과 블록 설계는 클록 속도를 50% 증가시킬 수 있었을 것입니다. K80과 같은 GDDR5 메모리를 위한 인터페이스 회로를 설계하면 Weight Memory 대역폭이 5배 이상 향상되어 루프라인 능선 지점을 1350에서 250으로 이동시킵니다. [Figure 11](#fig11)에서 볼 수 있듯이, 클록 속도를 1050 MHz로 증가시키고 메모리에 영향을 주지 않으면 거의 변화가 없습니다. 클록을 700 MHz에 한 두고 Weight Memory에 GDDR5를 사용하면 기하 평균이 2.6으로 증가하고 가중 평균 3.9로 증가합니다. 둘 다 수행하면 기하 평균(2.9)이 추가로 향상되지만 가중 평균이 그렇지는 않으므로 TPU’는 단지 더 빠른 메모리만 제공합니다.

[Figure 11](#fig11)은 호스트 서버 시간을 포함하지 않습니다. 우리는 [Table 5](#table5)를 사용하여 TPU의 호스트 서버 상호 작용 오버헤드에 대한 시간을 계산했습니다. 같은 추가 시간을 추가하면 TPU’ 평균이 2.6에서 1.9로, 3.9에서 3.2로 감소합니다. 이는 CPU가 응용 프로그램의 몫을 실행하는 시간을 포함하지 않아 낙관적이기도 하고, 3배 더 빠른 TPU’를 고려하여 호스트 코드를 공격적으로 튜닝할 가능성이 크기 때문에 비관적이기도 합니다.

DDR3 Weight Memory를 K80에 해당하는 GDDR5 메모리로 대체하는 것만으로도 메모리 채널의 수를 네 개로 두 배로 늘려야 합니다. 이는 다이 크기를 약 10% 확장시킬 것입니다. 그러나 더 높은 메모리 대역폭이 Unified Buffer에 대한 압력을 줄이므로Unified Buffer를 14 MiB로 줄이면 10%의 면적을 회복할 수 있습니다. GDDR5는 861 Watts의 TPU 시스템 전력 예산을 서버당 4개의 TPU 기준으로 약 900 Watts로 증가시킬 것입니다.

위의 [Figure 9](#fig9)은 TPU’의 상대 총성능/와트/다이가 Haswell 대비 31배~86배, K80 대비 25배~41배로 뛰어난 성능을 보여줍니다. 증분 지표는 Haswell 대비 69배~196배, K80 대비 42배~68배로 껑충 뛰어오릅니다.

# 8. Discussion

이 섹션은 [ref23]의 반박 스타일을 따릅니다.

<a name="sec8a"></a>

## • Fallacy: NN Inference Applications in Datacenters Value Throughput as Much as Response Time

개발자들이 응답 시간에 대한 강한 요구를 갖고 있다는 것은 놀라운 일이었습니다. 2014년에는 TPU가 최대 성능에 도달할 정도로 배치 크기가 클 것이거나 지연 시간 요구가 그렇게 엄격하지 않을 것이라는 의견도 있었습니다. 오프라인 이미지 처리가 주요 응용 프로그램 중 하나였으며, 직관적으로는 대다수의 대화형 서비스도 TPU를 원한다면 더 큰 배치를 누적할 것이라고 생각되었습니다. 2014년 한 응용 프로그램(LSTM1)의 개발자들도 응답 시간에 신경을 썼으며, 당시 최대 10ms까지 허용했으나, 실제로 TPU로 이식했을 때는 7ms로 줄였습니다. 이러한 많은 서비스들이 예상치 못하게 TPU를 원하게 되면서 낮은 응답 시간을 선호하게 되어, 응용 프로그램 작성자들은 종종 더 큰 배치를 기다리기보다 지연 시간을 줄이기를 선택했습니다. 다행히 TPU는 대화형 서비스의 응답 시간 목표를 충족시키는 데 도움이 되는 단순하고 반복 가능한 실행 모델을 가지고 있으며, 작은 배치 크기에서도 동시대의 CPU와 GPU보다 높은 성능을 발휘할 수 있을 정도로 높은 최고 처리량을 가지고 있습니다.

<a name="sec8b"></a>

## • Fallacy: The K80 GPU Architecture is a Good Match to NN Inference

GPU는 전통적으로 고대역폭 DRAM과 수천 개의 스레드를 통해 높은 처리량을 달성하는 아키텍처로 인식되어 왔습니다. 이러한 관점은 K80이 추론에서 Haswell보다 약간 빠르고 TPU보다 훨씬 느린 이유를 설명하는 데 도움이 됩니다. K80의 후속 제품은 최고의 추론 성능을 개선하기 위한 최적화를 포함할 것이 분명하지만, 처리량 중심의 아키텍처 접근 방식으로 인해 엄격한 지연 시간 한계를 충족하는 것이 더 어려울 수 있습니다. [섹션 7](#sec7)에서 보여준 것처럼 TPU를 개선할 수 있는 여지가 많이 있기 때문에 이는 쉽지 않은 목표입니다.

<a name="sec8c"></a>

## • Pitfall: Architects Neglected Important NN Tasks

아키텍처 커뮤니티가 NN에 주목하고 있는 것에 대해 기쁘게 생각합니다. ISCA 2016의 논문 중 15%가 NN 하드웨어 가속기와 관련되어 있었습니다 [2, 11, 13, 21, 29, 34, 35, 45, 52]! 아쉽게도 아홉 편의 논문 모두 CNN을 다뤘고, 그중 두 편만이 다른 NN을 언급했습니다. CNN은 MLP보다 복잡하고 NN 대회에서 두드러지게 보일 수 있지만, 데이터센터 NN 작업량의 약 5%에 불과합니다. CNN은 엣지 장치에서 일반적일 수 있지만, 데이터센터의 컨볼루션 모델의 양은 아직 MLP와 LSTMs에 비해 따라잡지 못했습니다. 아키텍트들이 MLP와 LSTMs의 가속화에 더 많은 열정을 쏟기를 바랍니다.

<a name="sec8d"></a>

## • Pitfall: For NN Hardware, Inferences Per Second (IPS) is an Inaccurate Summary Performance Metric

우리의 결과는 IPS가 NN 하드웨어에 대한 전체 성능 요약으로 부적절하다는 것을 보여줍니다. 이는 단지 응용 프로그램 내부의 일반적인 추론의 복잡성(예: NN 레이어의 수, 크기, 타입)의 역수에 불과하기 때문입니다. 예를 들어, TPU는 4층 MLP1을 360,000 IPS로 실행하지만, 89층 CNN1은 4,700 IPS로 실행합니다. 따라서 TPU IPS는 75배씩 변동합니다! 따라서 IPS를 단일 속도 요약으로 사용하는 것은 일반 프로세서를 위해 MIPS나 FLOPS를 사용하는 것보다 NN 가속기에 대해 훨씬 오도적이므로 IPS는 더욱 불신해야 합니다. NN 기계를 더 잘 비교하려면 다양한 NN 아키텍처에 포팅할 수 있는 고급 수준으로 작성된 벤치마크 스위트가 필요합니다. Fathom은 이러한 벤치마크 스위트에 대한 유망한 새로운 시도입니다 [ref3].

<a name="sec8e"></a>

## • Fallacy: The K80 GPU Results Would be Much Better if Boost Mode were Enabled

K80 부스트 모드를 사용하지 않았지만 LSTM1에 미치는 영향을 측정했습니다. 부스트 모드는 클럭 속도를 560에서 875 MHz로 최대 1.6배까지 증가시켰으며, 이는 성능을 1.4배 증가시켰지만, 전력 소모도 1.3배 증가시켰습니다. 성능/와트의 순 이득은 1.1배이며, 따라서 부스트 모드는 LSTM1에 미치는 영향이 미미할 것입니다.

<a name="sec8f"></a>

## • Fallacy: CPU and GPU Results Would be Similar to the TPU if we Used them More Efficiently or Compared to Newer Versions

처음에는 CPU에서 단일 DNN에 대해 8비트 결과만 있었고, 이는 AVX2 정수 지원을 효율적으로 사용하는 데 많은 노력 덕분이었습니다. 그 이점은 약 3.5배였습니다. 모든 CPU 결과를 부동 소수점으로 제시하는 것이 이례적인 경우와 그 자체의 루프라인을 피하는 데 더 명확하고 간편했습니다. 모든 DNN이 유사한 속도 향상을 보였다면, 성능/와트 비율은 41배–83배에서 12배–24배로 줄어들었을 것입니다. 새로운 16-nm, 1.5GHz, 250W P40 GPU는 초당 47 테라 8비트 연산을 수행할 수 있지만, 이는 2015년 초에는 사용 가능하지 않았으므로 우리의 세 플랫폼과 동시대의 것이 아닙니다. 또한 고정된 시간 한도 내에 P40의 최고 성능이 발휘될 비율을 알 수 없습니다. (또한 내부 메모리에 대한 SECDED를 제공하지 않으므로 데이터센터에 도입할 수 없습니다.) 우리가 최신 칩을 비교한다면, [섹션 7](#sec7)에서 보여주듯이 K80의 GDDR5 메모리를 사용함으로써 28-nm, 0.7GHz, 40W TPU의 성능을 세 배로 늘릴 수 있습니다(추가 10W 비용이 듭니다).

<a name="sec8g"></a>

## • Pitfall: Performance Counters Added as an Afterthought for NN Hardware

TPU에는 106개의 성능 카운터가 있으며, 더 많은 것을 원합니다 ([표 3](#table3)을 참조하세요). NN 가속기의 존재 이유는 성능이며, 아직 그들의 진화 초기 단계에서 무슨 일이 일어나고 있는지에 대한 좋은 직관을 갖기엔 너무 이릅니다.

<a name="sec8h"></a>

## • Fallacy: After Two Years of Software Tuning, the Only Path Left to Increase TPU Performance is Hardware Upgrades

TPU에서 CNN1의 성능은 개발자와 컴파일러 작성자들이 CNN1을 TPU 하드웨어에 더 잘 맞추기 위해 더 많은 작업을 했을 경우 개선될 수 있습니다. 예를 들어, 개발자는 여러 짧은 배치를 컨볼루션 레이어에서 단일의 더 깊은 배치로 집계하도록 응용 프로그램을 재구성할 수 있습니다(32에서 128로). 이러한 단일 레이어는 행렬 유닛의 활용도를 개선할 것입니다. CNN1은 현재 TPU에서 CPU보다 70배 이상 빠르게 실행되며, CNN1의 개발자들은 이미 매우 만족하고 있어 이러한 최적화가 수행될지 언제일지는 명확하지 않습니다.

<a name="sec8i"></a>

## • Pitfall: Being Ignorant of Architecture History When Designing a Domain-Specific Architecture

범용 컴퓨팅에 적합하지 않은 아이디어가 도메인별 아키텍처에는 이상적일 수 있습니다. TPU의 경우, 세 가지 중요한 아키텍처적 특징이 1980년대 초까지 거슬러 올라갑니다: 시스톨릭 배열 [ref31], 분리된 접근/실행 [ref54], 그리고 CISC 명령어 [ref41]. 첫 번째는 큰 행렬 곱셈 유닛의 면적과 전력을 줄였고, 두 번째는 행렬 곱셈 유닛 작동 중 가중치를 동시Fetch하였으며, 세 번째는 PCIe 버스의 제한된 대역폭을 더 잘 활용했습니다. 과거를 인지하는 아키텍트들은 경쟁 우위를 가질 수 있습니다.

<a name="sec9"></a>

# 9. Related Work

두 개의 조사 논문은 맞춤형 NN ASICs가 최소 25년 전으로 거슬러 올라간다는 것을 문서화하고 있습니다 [ref25], [ref4]. 예를 들어, CNAPS 칩은 16비트와 8비트의 곱셈기 64개로 구성된 SIMD 배열을 포함하고 있었으며, CNAPS 칩 여러 개를 시퀀서와 함께 연결할 수 있었습니다 [ref19]. Synapse-1 시스템은 MA-16이라 불리는 맞춤형 병렬 곱셈 누산 칩에 기반을 두고 있었으며, 이 칩은 동시에 16개의 16비트 곱셈을 수행했습니다 [ref44]. 이 시스템은 여러 개의 MA-16 칩을 연결하여 활성화 함수 수행을 위한 맞춤형 하드웨어를 포함했습니다.

25개의 SPERT-II 워크스테이션은 T0 맞춤형 ASIC으로 가속화되었으며, 1995년부터 음성 인식을 위한 NN 학습과 추론을 수행하기 위해 설치되었습니다 [ref5]. 40-Mhz T0는 MIPS 명령어 집합 아키텍처에 벡터 명령어를 추가했습니다. 8라인 벡터 유닛은 8비트와 16비트 입력을 기반으로 클럭 사이클 당 최대 16개의 32비트 산술 결과를 생산할 수 있었으며, 이는 SPARC-20 워크스테이션보다 추론에서는 25배, 학습에서는 20배 더 빠르게 만들었습니다. 그들은 학습에 16비트가 충분하지 않다는 것을 발견하였고, 대신 두 개의 16비트 단어를 사용하였으며, 이로 인해 학습 시간이 두 배로 늘어났습니다. 그 단점을 극복하기 위해, 그들은 32에서 1000개 데이터셋의 "버치" (배치) 를 도입하여 가중치 업데이트에 소요되는 시간을 줄였고, 이는 한 단어를 사용하여 학습하는 것에 비해 더 빠르게 만들었습니다.

최근의 DianNao 패밀리 NN 아키텍처는 효율적인 아키텍처 지원을 통해 NN 응용에서 발생하는 메모리 접근 패턴을 다루어, 칩 자체와 외부 DRAM으로의 메모리 접근을 최소화합니다 [ref28], [ref11]. 모든 디자인은 16비트 정수 연산을 사용하며, 모든 디자인이 레이아웃까지 합성되었으나 제조된 칩은 없습니다. 오리지널 DianNao는 44 KB의 온칩 메모리를 가지는 64개의 16비트 정수 곱셈-누산 유닛 배열을 사용하고, 3mm<sup>2</sup> (65 nm)에 1 GHz에서 구동되고 0.5W를 소비하는 것으로 추정됩니다. 대부분의 에너지는 가중치를 위한 DRAM 접근에 사용되었으며, 이에 따라 DaDianNao(“대형 컴퓨터”)는 36 MiB의 가중치를 온칩에 보관하기 위해 eDRAM을 포함하고 있습니다. 목표는 외부 DRAM 접근을 피할 수 있도록 멀티칩 시스템에 충분한 메모리를 갖는 것이었습니다. 후속 제품인 PuDianNao(“일반 컴퓨터”)는 주로 지원 벡터 머신을 겨냥하고 있습니다. 또 다른 파생 제품은 ShiDianNao(“비전 컴퓨터”)로 CNN을 목표로 하며, 엑셀러레이터가 센서에 직접 연결되어 DRAM 접근을 피합니다.

Convolution Engine은 이미지 처리용 CNN에 초점을 맞추고 있습니다 [ref43]. 이 디자인은 64개의 10비트 곱셈-누산 유닛을 배치하며, 45 nm에서 800 MHz로 실행될 것으로 추정되는 Tensilica 프로세서를 기본으로 맞춤화했습니다. 이는 SIMD 프로세서보다 8배에서 15배 더 에너지-면적 효율적이며, 특정 커널에 맞춰 디자인된 맞춤형 하드웨어에 비해 2배에서 3배 내에 있을 것으로 예상됩니다.

Fathom 벤치마크 논문은 GPU가 CPU보다 추론을 훨씬 빠르게 처리한다는, 우리의 결과와 모순된 결과를 보고하는 것처럼 보입니다 [ref3]. 그러나 그들의 CPU와 GPU는 서버급이 아니며, CPU는 단지 4개의 코어를 가지고 있고, 애플리케이션은 CPU의 AVX 명령어를 사용하지 않으며, 응답 시간 제한도 없습니다 [ref8].

Catapult [ref42]는 DNN을 지원하기 위한 재구성 가능성을 활용하는 가장 널리 배포된 예입니다. 그들은 전력 소비를 줄이기 위해 GPU 대신 FPGA를 선택했으며, 지연 시간에 민감한 응용 프로그램이 GPU에 잘 맞지 않을 위험도 줄였습니다. FPGA는 검색, 압축, 네트워크 인터페이스 카드 등 다목적으로 재설정될 수 있습니다 [Put15]. TPU 프로젝트는 실제로 FPGA로 시작했으나, 그 당시의 FPGA가 그 당시의 GPU와 비교했을 때 성능이 경쟁적이지 않다는 것을 볼 때 이를 포기했고, TPU는 GPU보다 훨씬 낮은 전력으로 그만큼 빠르거나 더 빠를 수 있었기 때문에 FPGA와 GPU보다 훨씬 더 나을 수 있었습니다.

비록 2014년에 처음 발표되었지만, Catapult는 2015년에 TPU와 동시에 데이터센터에 28-nm Stratix V FPGA를 배포하여 TPU 시대를 맞이한 것으로 볼 수 있습니다. Catapult는 200 MHz 클럭, 3,926개의 18비트 MACs, 5 MiB의 온칩 메모리, 11 GB/s 메모리 대역폭을 가지고 있으며, 25 와트를 사용합니다. TPU는 700 MHz 클럭, 65,536개의 8비트 MACs, 28 MiB, 34 GB/s 및 일반적으로 40 와트를 사용합니다. Catapult의 수정 버전은 2016년에 더 큰 규모로 배포되었습니다 [ref9].

Catapult V1은 CNNs을 사용하는-시스톨릭 행렬 곱셈기를 사용하는- 2.1 GHz, 16코어, 듀얼 소켓 서버보다 2.3배 빠르게 실행됩니다 [ref39]. Catapult V2의 FPGA의 다음 세대(14-nm Arria 10)를 사용하면, 성능이 최대 7배까지, 더 신중한 플로어 계획이 포함되면 아마도 17배까지 증가할 수 있습니다 [ref40]. 비록 비교가 어렵지만, 현재의 TPU 다이는 다소 더 빠른 서버 대비 CNNs을 40배에서 70배 더 빠르게 실행합니다 ([Tables 2](#table2) 및 [6](#table6)). 아마도 가장 큰 차이점은 최상의 성능을 얻기 위해 사용자가 저수준 하드웨어 디자인 언어 Verilog로 긴 프로그램을 작성해야 한다는 점이며, 이는 고급 TensorFlow 프레임워크를 사용하여 짧은 프로그램을 작성하는 것과 대비됩니다. 즉, 프로그래머블 성능은 TPU의 경우 소프트웨어에서 오는 반면, FPGA의 경우 펌웨어에서 옵니다.

# 10. Conclusion

I/O 버스 상에서 작동하고 메모리 대역폭이 상대적으로 낮아 TPU 사용률이 제한되는 상황에서, 여섯 개의 신경망(NN) 애플리케이션 중 네 개는 메모리 제한을 받습니다. 그러나 Roofline 성능 모델이 보여주듯이 큰 숫자의 작은 비율도 상대적으로 클 수 있습니다. 이 결과는 Amdahl's Law에 대한 "Cornucopia Corollary"를 시사합니다: 거대한, 저렴한 자원의 낮은 사용률도 높은, 비용 효과적인 성능을 제공할 수 있다는 것입니다.

TPU는 K80 GPU의 32-bit 부동소수점 데이터 경로에 비해 8-bit 정수 Systolic Matrix Multiplier가 제공하는 에너지 및 면적 감소를 활용하여, 25배 더 많은 MACs(65,536개의 8-bit 대 2,496개의 32-bit)와, 3.5배 많은 온칩 메모리(28 MiB 대 8 MiB)를 상대적으로 작은 칩에서 K80의 절반 이하의 전력을 사용하여 담고 있습니다. 이 더 큰 메모리는 애플리케이션의 운영 강도를 증가시켜, 이들이 풍부한 MACs를 더욱 완전히 활용할 수 있도록 도와줍니다.

최근 아키텍처 커뮤니티에서 CNNs에 중점을 두고 있음에도 불구하고, 데이터센터에서 대표적인 NN 워크로드는 CNNs이 약 5%에 불과하다는 것을 발견했습니다. 이는 MLPs와 LSTMs에 더 많은 주의를 기울여야 한다는 것을 시사합니다. 이는 과거 많은 설계자들이 주로 정수 연산에 지배되는 주류 워크로드에 부동소수점 성능을 집중했던 것과 유사합니다.

초당 추론 횟수(IPS)는 하드웨어보다는 신경망의 함수임을 관찰했습니다. 따라서 IPS는 CPU 및 GPU의 MIPS와 MFLOPS보다 NN 프로세서에 더 나쁜 단일 성능 지표입니다.

추론 애플리케이션은 사용자 대상 애플리케이션의 일부인 경우가 많아 반응 시간 제한이 심각하다는 것을 배웠습니다. 따라서 NN 아키텍처는 99번째 백분위수 지연 시간 마감 기한을 처리할 때도 잘 작동해야 합니다. K80은 학습에서는 뛰어날 수 있지만, 우리의 워크로드에서는 추론에서 Haswell보다 평균적으로 약간 빠릅니다. 이는 아마도 지연보다는 처리량을 강조하기 때문인지도 모릅니다; 이는 추론 애플리케이션의 엄격한 반응 시간 마감 기한과 상충됩니다.

TPU 다이는 MACs와 온칩 메모리의 이점을 활용하여 도메인 전용 TensorFlow 프레임워크를 사용해 작성된 짧은 프로그램을 K80 GPU 다이보다 15배 빠르게 실행시키며, 이는 성능 당 와트 성능에서 29배의 이점을 제공하며 이는 총 소유 비용 당 성능과 상관관계가 있습니다. Haswell CPU 다이와 비교했을 때, 해당 비율은 29와 83입니다. 미래의 CPU와 GPU는 추론을 더 빠르게 실행하겠지만, 2015년경의 GPU 메모리를 사용하는 새로 설계된 TPU는 두세 배 더 빠르게 작동할 수 있으며, K80 및 Haswell 대비 성능/Watt 이점을 각각 70 및 200에 가깝게 높일 수 있습니다.

요약하자면, TPU는 매우 큰-그러나 지나치게 크지 않은-매트릭스 곱셈 유닛; 상당한 소프트웨어 제어 온칩 메모리; 호스트 CPU 의존도를 줄이기 위해 전체 추론 모델을 실행할 수 있는 능력; 99번째 백분위수 반응 시간 제한에 잘 맞춘 단일 스레드, 결정론적 실행 모델; 2017년 뿐만 아니라 2013년의 NN과도 일치할 수 있는 충분한 유연성; 더 큰 데이터 경로와 메모리에도 불구하고 작고 낮은 전력의 칩을 가능하게 했던 범용 기능의 생략; 양자화된 애플리케이션에서 8-bit 정수의 사용; TensorFlow를 사용하여 작성되어 고성능으로 쉽게 TPU로 포팅할 수 있었던 애플리케이션들 덕분에 성공을 거두었습니다.

제품 간의 단위 차이는 컴퓨터 아키텍처에서 드문 현상으로, 이는 TPU가 도메인 특정 아키텍처의 전형이 될 수 있음을 암시합니다. 우리는 많은 사람들이 기준을 더욱 높일 후속 제품을 제작할 것으로 기대합니다.


